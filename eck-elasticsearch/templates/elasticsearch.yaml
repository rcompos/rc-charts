# This sample sets up an Elasticsearch cluster with 3 nodes.
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch
spec:
  version: {{ .Values.version }}
  nodeSets:
  # MASTER NODES
  # - name: default
  - name: master
    count: {{ .Values.masterCount }}
    config:
      # most Elasticsearch configuration parameters are possible to set, e.g: node.attr.attr_name: attr_value
      # node.roles: ["master", "data", "ingest", "ml"]
      node.roles: ["master"]
      # this allows ES to run on nodes even if their vm.max_map_count has not been increased, at a performance cost
      node.store.allow_mmap: false
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          # foo: bar
          app: efk
      spec:
        # this changes the kernel setting on the node to allow ES to use mmap
        # if you uncomment this init container you will likely also want to remove the
        # "node.store.allow_mmap: false" setting above
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        ###
        # uncomment the line below if you are using a service mesh such as linkerd2 that uses service account tokens for pod identification.
        # automountServiceAccountToken: true
        containers:
        - name: elasticsearch
          # specify resource limits and requests
          resources:
            {{- toYaml .Values.master.resources | nindent 12 }}
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms2g -Xmx2g"
        {{- with .Values.master.nodeSelector }}
        nodeSelector:
          {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- with .Values.master.affinity }}
        affinity:
          {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- with .Values.master.tolerations }}
        tolerations:
          {{- toYaml . | nindent 8 }}
        {{- end }}
  #   # request 2Gi of persistent data storage for pods in this topology element
  #   volumeClaimTemplates:
  #   - metadata:
  #       name: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.
  #     spec:
  #       accessModes:
  #       - ReadWriteOnce
  #       resources:
  #         requests:
  #           storage: 2Gi
  #       storageClassName: standard
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {{ .Values.master.volumeSize }}
       # storageClassName: gp2
  # DATA NODES
  - name: data
    count: {{ .Values.dataCount }}
    config:
      # most Elasticsearch configuration parameters are possible to set, e.g: node.attr.attr_name: attr_value
      node.roles: ["data", "ingest", "ml"]
      # this allows ES to run on nodes even if their vm.max_map_count has not been increased, at a performance cost
      node.store.allow_mmap: false
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          # foo: bar
          app: eck
      spec:
        # this changes the kernel setting on the node to allow ES to use mmap
        # if you uncomment this init container you will likely also want to remove the
        # "node.store.allow_mmap: false" setting above
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        ###
        # uncomment the line below if you are using a service mesh such as linkerd2 that uses service account tokens for pod identification.
        # automountServiceAccountToken: true
        containers:
        - name: elasticsearch
          # specify resource limits and requests
          resources:
            {{- toYaml .Values.data.resources | nindent 12 }}
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms2g -Xmx2g"
        {{- with .Values.data.nodeSelector }}
        nodeSelector:
          {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- with .Values.data.affinity }}
        affinity:
          {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- with .Values.data.tolerations }}
        tolerations:
          {{- toYaml . | nindent 8 }}
        {{- end }}
  #   # request 2Gi of persistent data storage for pods in this topology element
  #   volumeClaimTemplates:
  #   - metadata:
  #       name: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.
  #     spec:
  #       accessModes:
  #       - ReadWriteOnce
  #       resources:
  #         requests:
  #           storage: 2Gi
  #       storageClassName: standard
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {{ .Values.data.volumeSize }}
        # storageClassName: gp2

  # # inject secure settings into Elasticsearch nodes from k8s secrets references
  # secureSettings:
  # - secretName: ref-to-secret
  # - secretName: another-ref-to-secret
  #   # expose only a subset of the secret keys (optional)
  #   entries:
  #   - key: value1
  #     path: newkey # project a key to a specific path (optional)
  # http:
  #   service:
  #     spec:
  #       # expose this cluster Service with a LoadBalancer
  #       type: LoadBalancer
  http:
  #   service:
  #     spec:
  #       # expose this cluster Service with a LoadBalancer
  #       type: LoadBalancer
    tls: {}
    # tls:
    #  selfSignedCertificate:
    #    disabled: true
    #     # add a list of SANs into the self-signed HTTP certificate
    #     subjectAltNames:
    #     - ip: 172.0.0.0
    #     - dns: elasticsearch.sandbox.dev.data.pdx.atieva.com
  #     certificate:
  #       # provide your own certificate
  #       secretName: my-cert

